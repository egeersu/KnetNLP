{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-based Neural Machine Translation\n",
    "\n",
    "**Reference:** Luong, Thang, Hieu Pham and Christopher D. Manning. \"Effective Approaches to Attention-based Neural Machine Translation.\" In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1412-1421. 2015.\n",
    "\n",
    "* https://www.aclweb.org/anthology/D15-1166/ (main paper reference)\n",
    "* https://arxiv.org/abs/1508.04025 (alternative paper url)\n",
    "* https://github.com/tensorflow/nmt (main code reference)\n",
    "* https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention (alternative code reference)\n",
    "* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py:2449,2103 (attention implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import Pkg;\n",
    "packages = [\"Knet\", \"Test\", \"Printf\", \"LinearAlgebra\", \"Random\", \"CuArrays\", \"IterTools\"]\n",
    "for p in packages; Pkg.add(p); end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Test, Base.Iterators, Printf, LinearAlgebra, Random, CuArrays, IterTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and data from previous projects\n",
    "\n",
    "Please copy or include the following types and related functions from previous projects:\n",
    "`Vocab`, `TextReader`, `MTData`, `Embed`, `Linear`, `mask!`, `loss`, `int2str`,\n",
    "`bleu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "include(\"hw3_functions.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2S: Sequence to sequence model with attention\n",
    "\n",
    "In this project we will define, train and evaluate a sequence to sequence encoder-decoder\n",
    "model with attention for Turkish-English machine translation. The model has two extra\n",
    "fields compared to `S2S_v1`: the `memory` layer computes keys and values from the encoder,\n",
    "the `attention` layer computes the attention vector for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Memory; w; end\n",
    "\n",
    "struct Attention; wquery; wattn; scale; end\n",
    "\n",
    "struct S2S\n",
    "    srcembed::Embed       # encinput(B,Tx) -> srcembed(Ex,B,Tx)\n",
    "    encoder::RNN          # srcembed(Ex,B,Tx) -> enccell(Dx*H,B,Tx)\n",
    "    memory::Memory        # enccell(Dx*H,B,Tx) -> keys(H,Tx,B), vals(Dx*H,Tx,B)\n",
    "    tgtembed::Embed       # decinput(B,Ty) -> tgtembed(Ey,B,Ty)\n",
    "    decoder::RNN          # tgtembed(Ey,B,Ty) . attnvec(H,B,Ty)[t-1] = (Ey+H,B,Ty) -> deccell(H,B,Ty)\n",
    "    attention::Attention  # deccell(H,B,Ty), keys(H,Tx,B), vals(Dx*H,Tx,B) -> attnvec(H,B,Ty)\n",
    "    projection::Linear    # attnvec(H,B,Ty) -> proj(Vy,B,Ty)\n",
    "    dropout::Real         # dropout probability\n",
    "    srcvocab::Vocab       # source language vocabulary\n",
    "    tgtvocab::Vocab       # target language vocabulary\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model and data\n",
    "\n",
    "We will load a pretrained model (16.20 bleu) for code testing.  The data should be loaded\n",
    "with the vocabulary from the pretrained model for word id consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading reference model\n",
      "└ @ Main In[5]:2\n",
      "┌ Info: Reading data\n",
      "└ @ Main In[5]:14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MTData(TextReader(\"datasets/tr_to_en/tr.test\", Vocab(Dict(\"dev\" => 1277,\"komuta\" => 13566,\"ellisi\" => 25239,\"adresini\" => 22820,\"yüzeyi\" => 4051,\"paris'te\" => 9494,\"kafamdaki\" => 18790,\"yüzeyinde\" => 5042,\"geçerlidir\" => 6612,\"kökten\" => 7774…), [\"<s>\", \"<unk>\", \".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\"  …  \"seçmemiz\", \"destekleyip\", \"karşılaştırılabilir\", \"ördeğin\", \"gününüzü\", \"bağışçı\", \"istismara\", \"yaşça\", \"tedci\", \"fakültesi'nde\"], 2, 1, split)), TextReader(\"datasets/tr_to_en/en.test\", Vocab(Dict(\"middle-income\" => 13398,\"photosynthesis\" => 7689,\"polarizing\" => 17881,\"henry\" => 4248,\"abducted\" => 15691,\"rises\" => 6225,\"hampshire\" => 13888,\"whiz\" => 16835,\"cost-benefit\" => 13137,\"progression\" => 5549…), [\"<s>\", \"<unk>\", \",\", \".\", \"the\", \"and\", \"to\", \"of\", \"a\", \"that\"  …  \"archaea\", \"handshake\", \"brit\", \"wiper\", \"heroines\", \"coca\", \"exceptionally\", \"gallbladder\", \"autopsies\", \"linguistics\"], 2, 1, split)), 64, 9223372036854775807, false, 10, Array{Any,1}[[], [], [], [], [], [], [], [], [], []  …  [], [], [], [], [], [], [], [], [], []], arraybatch)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if !isdefined(Main, :pretrained) || pretrained === nothing\n",
    "    @info \"Loading reference model\"\n",
    "    isfile(\"s2smodel.jld2\") || download(\"http://people.csail.mit.edu/deniz/comp542/s2smodel.jld2\",\"s2smodel.jld2\")\n",
    "    pretrained = Knet.load(\"s2smodel.jld2\",\"model\")\n",
    "end\n",
    "datadir = \"datasets/tr_to_en\"\n",
    "if !isdir(datadir)\n",
    "    @info \"Downloading data\"\n",
    "    download(\"http://www.phontron.com/data/qi18naacl-dataset.tar.gz\", \"qi18naacl-dataset.tar.gz\")\n",
    "    run(`tar xzf qi18naacl-dataset.tar.gz`)\n",
    "end\n",
    "if !isdefined(Main, :tr_vocab)\n",
    "    BATCHSIZE, MAXLENGTH = 64, 50\n",
    "    @info \"Reading data\"\n",
    "    tr_vocab = pretrained.srcvocab # Vocab(\"$datadir/tr.train\", mincount=5)\n",
    "    en_vocab = pretrained.tgtvocab # Vocab(\"$datadir/en.train\", mincount=5)\n",
    "    tr_train = TextReader(\"$datadir/tr.train\", tr_vocab)\n",
    "    en_train = TextReader(\"$datadir/en.train\", en_vocab)\n",
    "    tr_dev = TextReader(\"$datadir/tr.dev\", tr_vocab)\n",
    "    en_dev = TextReader(\"$datadir/en.dev\", en_vocab)\n",
    "    tr_test = TextReader(\"$datadir/tr.test\", tr_vocab)\n",
    "    en_test = TextReader(\"$datadir/en.test\", en_vocab)\n",
    "    dtrn = MTData(tr_train, en_train, batchsize=BATCHSIZE, maxlength=MAXLENGTH)\n",
    "    ddev = MTData(tr_dev, en_dev, batchsize=BATCHSIZE)\n",
    "    dtst = MTData(tr_test, en_test, batchsize=BATCHSIZE)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1. Model constructor\n",
    "\n",
    "The `S2S` constructor takes the following arguments:\n",
    "* `hidden`: size of the hidden vectors for both the encoder and the decoder\n",
    "* `srcembsz`, `tgtembsz`: size of the source/target language embedding vectors\n",
    "* `srcvocab`, `tgtvocab`: the source/target language vocabulary\n",
    "* `layers=1`: number of layers\n",
    "* `bidirectional=false`: whether the encoder is bidirectional\n",
    "* `dropout=0`: dropout probability\n",
    "\n",
    "Hints:\n",
    "* You can find the vocabulary size with `length(vocab.i2w)`.\n",
    "* If the encoder is bidirectional `layers` must be even and the encoder should have `layers÷2` layers.\n",
    "* The decoder will use \"input feeding\", i.e. it will concatenate its previous output to its input. Therefore the input size for the decoder should be `tgtembsz+hidden`.\n",
    "* Only `numLayers`, `dropout`, and `bidirectional` keyword arguments should be used for RNNs, leave everything else default.\n",
    "* The memory parameter `w` is used to convert encoder states to keys. If the encoder is bidirectional initialize it to a `(hidden,2*hidden)` parameter, otherwise set it to the constant 1.\n",
    "* The attention parameter `wquery` is used to transform the query, set it to the constant 1 for this project.\n",
    "* The attention parameter `scale` is used to scale the attention scores before softmax, set it to a parameter of size 1.\n",
    "* The attention parameter `wattn` is used to transform the concatenation of the decoder output and the context vector to the attention vector. It should be a parameter of size `(hidden,2*hidden)` if unidirectional, `(hidden,3*hidden)` if bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function S2S(hidden::Int, srcembsz::Int, tgtembsz::Int, srcvocab::Vocab, tgtvocab::Vocab;\n",
    "             layers=1, bidirectional=false, dropout=0)\n",
    "    # Your code here\n",
    "#     srcembed::Embed       # encinput(B,Tx) -> srcembed(Ex,B,Tx)\n",
    "#     encoder::RNN          # srcembed(Ex,B,Tx) -> enccell(Dx*H,B,Tx)\n",
    "#     memory::Memory        # enccell(Dx*H,B,Tx) -> keys(H,Tx,B), vals(Dx*H,Tx,B)\n",
    "#     tgtembed::Embed       # decinput(B,Ty) -> tgtembed(Ey,B,Ty)\n",
    "#     decoder::RNN          # tgtembed(Ey,B,Ty) . attnvec(H,B,Ty)[t-1] = (Ey+H,B,Ty) -> deccell(H,B,Ty)\n",
    "#     attention::Attention  # deccell(H,B,Ty), keys(H,Tx,B), vals(Dx*H,Tx,B) -> attnvec(H,B,Ty)\n",
    "#     projection::Linear    # attnvec(H,B,Ty) -> proj(Vy,B,Ty)\n",
    "#     dropout::Real         # dropout probability\n",
    "#     srcvocab::Vocab       # source language vocabulary\n",
    "#     tgtvocab::Vocab       # target language vocabulary\n",
    "    \n",
    "    # embedding\n",
    "    vocab_size_source = length(srcvocab.i2w)\n",
    "    vocab_size_target = length(tgtvocab.i2w)\n",
    "    srcembed = Embed(vocab_size_source, srcembsz)\n",
    "    tgtembed = Embed(vocab_size_target, tgtembsz)\n",
    "    if bidirectional\n",
    "        encoder=RNN(srcembsz, hidden, numLayers=1/2*layers, dropout=dropout, bidirectional=bidirectional)\n",
    "        memory=Memory(param(hidden,2*hidden))\n",
    "        wattn=param(hidden,3*hidden)        \n",
    "    else\n",
    "        memory=Memory(1)\n",
    "        wattn=param(hidden,2*hidden)\n",
    "    end\n",
    "    \n",
    "    wquery=1\n",
    "    scale=param(1)\n",
    "    encoder=RNN(srcembsz, hidden, numLayers=1/2*layers, dropout=dropout, bidirectional=bidirectional)\n",
    "    decoder=RNN(tgtembsz+hidden, hidden, numLayers=layers, dropout=dropout, bidirectional=bidirectional)\n",
    "   \n",
    "    attention=Attention(wquery, wattn, scale)\n",
    "    \n",
    "    projection=Linear(hidden, vocab_size_target)    \n",
    "    S2S(srcembed, encoder, memory, tgtembed, decoder, attention, projection, dropout, srcvocab, tgtvocab)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:           | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing S2S constructor | \u001b[32m  16  \u001b[39m\u001b[36m   16\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing S2S constructor\", Any[], 16, false)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@testset \"Testing S2S constructor\" begin\n",
    "    H,Ex,Ey,Vx,Vy,L,Dx,Pdrop = 8,9,10,length(dtrn.src.vocab.i2w),length(dtrn.tgt.vocab.i2w),2,2,0.2\n",
    "    m = S2S(H,Ex,Ey,dtrn.src.vocab,dtrn.tgt.vocab;layers=L,bidirectional=(Dx==2),dropout=Pdrop)\n",
    "    @test size(m.srcembed.w) == (Ex,Vx)\n",
    "    @test size(m.tgtembed.w) == (Ey,Vy)\n",
    "    @test m.encoder.inputSize == Ex\n",
    "    @test m.decoder.inputSize == Ey + H\n",
    "    @test m.encoder.hiddenSize == m.decoder.hiddenSize == H\n",
    "    @test m.encoder.direction == Dx-1\n",
    "    @test m.encoder.numLayers == (Dx == 2 ? L÷2 : L)\n",
    "    @test m.decoder.numLayers == L\n",
    "    @test m.encoder.dropout == m.decoder.dropout == Pdrop\n",
    "    @test size(m.projection.w) == (Vy,H)\n",
    "    @test size(m.memory.w) == (Dx == 2 ? (H,2H) : ())\n",
    "    @test m.attention.wquery == 1\n",
    "    @test size(m.attention.wattn) == (Dx == 2 ? (H,3H) : (H,2H))\n",
    "    @test size(m.attention.scale) == (1,)\n",
    "    @test m.srcvocab === dtrn.src.vocab\n",
    "    @test m.tgtvocab === dtrn.tgt.vocab\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Memory\n",
    "\n",
    "The memory layer turns the output of the encoder to a pair of tensors that will be used as\n",
    "keys and values for the attention mechanism. Remember that the encoder RNN output has size\n",
    "`(H*D,B,Tx)` where `H` is the hidden size, `D` is 1 for unidirectional, 2 for\n",
    "bidirectional, `B` is the batchsize, and `Tx` is the sequence length. It will be\n",
    "convenient to store these values in batch major form for the attention mechanism, so\n",
    "*values* in memory will be a permuted copy of the encoder output with size `(H*D,Tx,B)`\n",
    "(see `@doc permutedims`). The *keys* in the memory need to have the same first dimension\n",
    "as the *queries* (i.e. the decoder hidden states). So *values* will be transformed into\n",
    "*keys* of size `(H,B,Tx)` with `keys = m.w * values` where `m::Memory` is the memory\n",
    "layer. Note that you will have to do some reshaping to 2-D and back to 3-D for matrix\n",
    "multiplications. Also note that `m.w` may be a scalar such as `1` e.g. when `D=1` and we\n",
    "want keys and values to be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (m::Memory)(x)\n",
    "    # Your code here\n",
    "    values=permutedims(x, (1,3,2))\n",
    "#     print(size(values))\n",
    "#     print(size(m.w))\n",
    "    keys=mmul(m.w, values)\n",
    "    return keys, values\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following helper function for scaling and linear transformations of 3-D tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mmul (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmul(w,x) = (w == 1 ? x : w == 0 ? 0 : reshape(w * reshape(x,size(x,1),:), (:, size(x)[2:end]...)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:  | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing memory | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing memory\", Any[], 2, false)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@testset \"Testing memory\" begin\n",
    "    H,D,B,Tx = pretrained.encoder.hiddenSize, pretrained.encoder.direction+1, 4, 5\n",
    "    x = KnetArray(randn(Float32,H*D,B,Tx))\n",
    "    k,v = pretrained.memory(x)\n",
    "    @test v == permutedims(x,(1,3,2))\n",
    "    @test k == mmul(pretrained.memory.w, v)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Encoder\n",
    "\n",
    "`encode()` takes a model `s` and a source language minibatch `src`. It passes the input\n",
    "through `s.srcembed` and `s.encoder` layers with the `s.encoder` RNN hidden states\n",
    "initialized to `0` in the beginning, and copied to the `s.decoder` RNN at the end. The\n",
    "steps so far are identical to `S2S_v1` but there is an extra step: The encoder output is\n",
    "passed to the `s.memory` layer which returns a `(keys,values)` pair. `encode()` returns\n",
    "this pair to be used later by the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encode (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function encode(s::S2S, src)\n",
    "    # Your code here\n",
    "    # init encoder\n",
    "    s.encoder.h = 0\n",
    "    s.encoder.c = 0\n",
    "\n",
    "    # ENCODER\n",
    "    src_embed_out = s.srcembed(src)  \n",
    "    #println(\"source embed out: \", summary(src_embed_out), size(src_embed_out))\n",
    "    encoder_out = s.encoder(src_embed_out)\n",
    "    #println(\"encoder_out:\" , summary(encoder_out), size(encoder_out))\n",
    "    # init decoder with encoder's h and c\n",
    "    s.decoder.h = s.encoder.h\n",
    "    s.decoder.c = s.encoder.c\n",
    "#     print(s.decoder.c)\n",
    "    s.memory(encoder_out)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:   | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing encoder | \u001b[32m   7  \u001b[39m\u001b[36m    7\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing encoder\", Any[], 7, false)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@testset \"Testing encoder\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    key1,val1 = encode(pretrained, src1)\n",
    "    H,D,B,Tx = pretrained.encoder.hiddenSize, pretrained.encoder.direction+1, size(src1,1), size(src1,2)\n",
    "    @test size(key1) == (H,Tx,B)\n",
    "    @test size(val1) == (H*D,Tx,B)\n",
    "    @test (pretrained.decoder.h,pretrained.decoder.c) === (pretrained.encoder.h,pretrained.encoder.c)\n",
    "    @test norm(key1) ≈ 1214.4755f0\n",
    "    @test norm(val1) ≈ 191.10411f0\n",
    "    @test norm(pretrained.decoder.h) ≈ 48.536964f0\n",
    "    @test norm(pretrained.decoder.c) ≈ 391.69028f0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Attention\n",
    "\n",
    "The attention layer takes `cell`: the decoder output, and `mem`: a pair of (keys,vals)\n",
    "from the encoder, and computes and returns the attention vector. First `a.wquery` is used\n",
    "to linearly transform the cell to the query tensor. The query tensor is reshaped and/or\n",
    "permuted as appropriate and multiplied with the keys tensor to compute the attention\n",
    "scores. Please see `@doc bmm` for the batched matrix multiply operation used for this\n",
    "step. The attention scores are scaled using `a.scale` and normalized along the time\n",
    "dimension using `softmax`. After the appropriate reshape and/or permutation, the scores\n",
    "are multiplied with the `vals` tensor (using `bmm` again) to compute the context\n",
    "tensor. After the appropriate reshape and/or permutation the context vector is\n",
    "concatenated with the cell and linearly transformed to the attention vector using\n",
    "`a.wattn`. Please see the paper and code examples for details.\n",
    "\n",
    "Note: the paper mentions a final `tanh` transform, however the final version of the\n",
    "reference code does not use `tanh` and gets better results. Therefore we will skip `tanh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (a::Attention)(cell, mem)\n",
    "    # Your code here\n",
    "    query_tensor=a.wquery*cell\n",
    "    keys, vals=mem\n",
    "#     print(\"query tensor:\", size(query_tensor))\n",
    "#     print(\"keys:\", size(keys))\n",
    "    query_tensor=permutedims(mmul(a.wquery, cell), (3,1,2)) #shape 512,64,5\n",
    "    attention_scores=bmm(query_tensor, keys) #shape 5,20,64\n",
    "#     print(\"attention:\", size(attention_scores))\n",
    "    scaled_attention_scores=attention_scores* a.scale[1]\n",
    "    normalized_attention_scores=softmax(scaled_attention_scores, dims=2)\n",
    "    permuted_vals=permutedims(vals, (2,1,3)) #shape 20, 1024, 64\n",
    "    \n",
    "    context_tensor=bmm(normalized_attention_scores, permuted_vals)\n",
    "#     print(\"vals:\", size(vals))\n",
    "    permuted_context_tensor=permutedims(context_tensor, (2,3,1))\n",
    "#     concatenate context_tensor with the cell\n",
    "    concatenated_context_tensor=vcat(cell, permuted_context_tensor)\n",
    "    reshaped_context_tensor=reshape(concatenated_context_tensor, (size(a.wattn, 2),:))\n",
    "#     linearly transformed to the attention vector using a.wattn\n",
    "    transformed_context_tensor=a.wattn*reshaped_context_tensor\n",
    "    final_reshaped_context_tensor=reshape(transformed_context_tensor, size(cell))\n",
    "    return final_reshaped_context_tensor\n",
    "  \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:     | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing attention | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing attention\", Any[], 2, false)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@testset \"Testing attention\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    key1,val1 = encode(pretrained, src1)\n",
    "    H,B = pretrained.encoder.hiddenSize, size(src1,1)\n",
    "    Knet.seed!(1)\n",
    "    x = KnetArray(randn(Float32,H,B,5))\n",
    "    y = pretrained.attention(x, (key1, val1))\n",
    "    @test size(y) == size(x)\n",
    "    @test norm(y) ≈ 808.381f0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Decoder\n",
    "\n",
    "`decode()` takes a model `s`, a target language minibatch `tgt`, the memory from the\n",
    "encoder `mem` and the decoder output from the previous time step `prev`. After the input\n",
    "is passed through the embedding layer, it is concatenated with `prev` (this is called\n",
    "input feeding). The resulting tensor is passed through `s.decoder`. Finally the\n",
    "`s.attention` layer takes the decoder output and the encoder memory to compute the\n",
    "\"attention vector\" which is returned by `decode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decode (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function decode(s::S2S, tgt, mem, prev)\n",
    "    # Your code here\n",
    "    # DECODER\n",
    "    tgt_embed_out = s.tgtembed(tgt)\n",
    "    tgt_embed_out = reshape(tgt_embed_out, size(tgt_embed_out)[1], size(tgt_embed_out)[2], 1)\n",
    "    #println(\"tgt_embed_out: \", tgt_embed_out)\n",
    "    input_feeding=vcat(tgt_embed_out, prev)\n",
    "    #println(\"input_feeding: \", input_feeding)\n",
    "    decoder_output=s.decoder(input_feeding)\n",
    "    #println(\"decoder output: \", size(decoder_output))\n",
    "    attention_vector=s.attention(decoder_output, mem)\n",
    "    #println(\"attention vector: \", size(attention_vector))\n",
    "    return attention_vector\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:   | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing decoder | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing decoder\", Any[], 2, false)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@testset \"Testing decoder\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    key1,val1 = encode(pretrained, src1)\n",
    "    H,B = pretrained.encoder.hiddenSize, size(src1,1)\n",
    "    Knet.seed!(1)\n",
    "    cell = randn!(similar(key1, size(key1,1), size(key1,3), 1))\n",
    "    cell = decode(pretrained, tgt1[:,1:1], (key1,val1), cell)\n",
    "    @test size(cell) == (H,B,1)\n",
    "    @test norm(cell) ≈ 131.21631f0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Loss\n",
    "\n",
    "The loss function takes source language minibatch `src`, and a target language minibatch\n",
    "`tgt` and returns `sumloss/numwords` if `average=true` or `(sumloss,numwords)` if\n",
    "`average=false` where `sumloss` is the total negative log likelihood loss and `numwords` is\n",
    "the number of words predicted (including a final eos for each sentence). The source is first\n",
    "encoded using `encode` yielding a `(keys,vals)` pair (memory). Then the decoder is called to\n",
    "predict each word of `tgt` given the previous word, `(keys,vals)` pair, and the previous\n",
    "decoder output. The previous decoder output is initialized with zeros for the first\n",
    "step. The output of the decoder at each step is passed through the projection layer giving\n",
    "word scores. Losses can be computed from word scores and masked/shifted `tgt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S)(src, tgt; average=true)\n",
    "    # Your code here\n",
    "    \n",
    "    mem=encode(s, src)\n",
    "    key, val=mem\n",
    "    hidden=s.decoder.hiddenSize\n",
    "    batchSize = size(src,1)\n",
    "    targetLength = size(tgt,2)\n",
    "    prev = zeros(Float32, size(s.encoder.h, 1), batchSize, 1)\n",
    "    if (gpu() >= 0)\n",
    "        prev = KnetArray(prev)\n",
    "    end\n",
    "    sumloss=0\n",
    "    numwords=0\n",
    "    \n",
    "    to_be_masked=copy(tgt[:,2:end])\n",
    "    mask!(to_be_masked, s.tgtvocab.eos)\n",
    "    masked=to_be_masked\n",
    "    \n",
    "    for i in 1:targetLength-1\n",
    "        decoder_output=decode(s, tgt[:, i], mem, prev)\n",
    "        #println(\"decoder_output: \", size(decoder_output))\n",
    "        prev=decoder_output\n",
    "        reshaped_decoder_output=reshape(decoder_output, :, batchSize)\n",
    "        score=s.projection(reshaped_decoder_output)\n",
    "        delta_loss, delta_numwords=nll(score, masked[:, i], average=false)\n",
    "        sumloss=sumloss+delta_loss\n",
    "        numwords=numwords+delta_numwords\n",
    "    end\n",
    "    \n",
    "    if average\n",
    "        return sumloss/numwords\n",
    "    else\n",
    "        return sumloss,numwords\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mTesting loss: \u001b[39m\u001b[91m\u001b[1mTest Failed\u001b[22m\u001b[39m at \u001b[39m\u001b[1mIn[18]:4\u001b[22m\n",
      "  Expression: pretrained(src1, tgt1, average=false) == (1949.1901f0, 1329)\n",
      "   Evaluated: (1949.19f0, 1329) == (1949.1901f0, 1329)\n",
      "Stacktrace:\n",
      " [1] top-level scope at \u001b[1mIn[18]:4\u001b[22m\n",
      " [2] top-level scope at \u001b[1m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Test/src/Test.jl:1113\u001b[22m\n",
      " [3] top-level scope at \u001b[1mIn[18]:2\u001b[22m\n",
      "\u001b[37m\u001b[1mTest Summary: | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[91m\u001b[1mFail  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing loss  | \u001b[32m   1  \u001b[39m\u001b[91m   1  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "ename": "TestSetException",
     "evalue": "Some tests did not pass: 1 passed, 1 failed, 0 errored, 0 broken.",
     "output_type": "error",
     "traceback": [
      "Some tests did not pass: 1 passed, 1 failed, 0 errored, 0 broken.",
      "",
      "Stacktrace:",
      " [1] finish(::Test.DefaultTestSet) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Test/src/Test.jl:877",
      " [2] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Test/src/Test.jl:1123",
      " [3] top-level scope at In[18]:2"
     ]
    }
   ],
   "source": [
    "@testset \"Testing loss\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    @test pretrained(src1,tgt1) ≈ 1.4666592f0\n",
    "    @test pretrained(src1,tgt1,average=false) == (1949.1901f0, 1329)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7. Greedy translator\n",
    "\n",
    "An `S2S` object can be called with a single argument (source language minibatch `src`, with\n",
    "size `B,Tx`) to generate translations (target language minibatch with size `B,Ty`). The\n",
    "keyword argument `stopfactor` determines how much longer the output can be compared to the\n",
    "input. Similar to the loss function, the source minibatch is encoded yield a `(keys,vals)`\n",
    "pair (memory). We generate the output one time step at a time by calling the decoder with\n",
    "the last output, the memory, and the last decoder state. The last output is initialized to\n",
    "an array of `eos` tokens and the last decoder state is initialized to an array of\n",
    "zeros. After computing the scores for the next word using the projection layer, the highest\n",
    "scoring words are selected and appended to the output. The generation stops when all outputs\n",
    "in the batch have generated `eos` or when the length of the output is `stopfactor` times the\n",
    "input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S)(src; stopfactor = 3)\n",
    "    # Your code here\n",
    "    \n",
    "    isDone = false\n",
    "    batch_size = size(src,1)\n",
    "    input = repeat([s.tgtvocab.eos], batch_size)\n",
    "    is_all_finished = zeros(batch_size)\n",
    "    translated_sentences = copy(input)\n",
    "    max_length_output = 0\n",
    "    \n",
    "    mem = encode(s, src)\n",
    "    \n",
    "    prev_decoder_output = zeros(Float32, size(s.encoder.h, 1), batch_size, 1)\n",
    "    if (gpu() >= 0)\n",
    "        prev_decoder_output = KnetArray(prev_decoder_output)\n",
    "    end\n",
    "    input = reshape(input, (length(input), 1))\n",
    "    \n",
    "    while (!isDone && max_length_output < stopfactor*size(src,2))        \n",
    "        \n",
    "        \n",
    "        y = decode(s, input, mem, prev_decoder_output)\n",
    "        prev_decoder_output = y\n",
    "        \n",
    "          \n",
    "        hy, b ,ty = size(y)\n",
    "        y = reshape(y, (hy, b*ty))\n",
    "        \n",
    "        scores = s.projection(y)\n",
    "        \n",
    "        output_words = reshape(map(x->x[1], argmax(scores, dims = 1)), batch_size)\n",
    "        translated_sentences = hcat(translated_sentences, output_words)\n",
    "       \n",
    "        max_length_output = size(translated_sentences, 2)\n",
    "        input = reshape(output_words, (length(output_words), 1))\n",
    "        \n",
    "       \n",
    "        tmp_output_words = copy(output_words)\n",
    "        tmp_output_words = tmp_output_words .== s.tgtvocab.eos\n",
    "        is_all_finished += tmp_output_words\n",
    "        if(sum(is_all_finished.==0)==0)\n",
    "            isDone = true\n",
    "        end\n",
    "    end\n",
    "    return translated_sentences[:, 2:end]\n",
    "end   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[1mTest Summary:      | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "Testing translator | \u001b[32m   2  \u001b[39m\u001b[36m    2\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Testing translator\", Any[], 2, false)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@testset \"Testing translator\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    tgt2 = pretrained(src1)\n",
    "    @test size(tgt2) == (64, 41)\n",
    "    @test tgt2[1:3,1:3] == [14 25 10647; 37 25 1426; 27 5 349]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8. Training\n",
    "\n",
    "`trainmodel` creates, trains and returns an `S2S` model. The arguments are described in\n",
    "comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainmodel (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function trainmodel(trn,                  # Training data\n",
    "                    dev,                  # Validation data, used to determine the best model\n",
    "                    tst...;               # Zero or more test datasets, their loss will be periodically reported\n",
    "                    bidirectional = true, # Whether to use a bidirectional encoder\n",
    "                    layers = 2,           # Number of layers (use `layers÷2` for a bidirectional encoder)\n",
    "                    hidden = 512,         # Size of the hidden vectors\n",
    "                    srcembed = 512,       # Size of the source language embedding vectors\n",
    "                    tgtembed = 512,       # Size of the target language embedding vectors\n",
    "                    dropout = 0.2,        # Dropout probability\n",
    "                    epochs = 0,           # Number of epochs (one of epochs or iters should be nonzero for training)\n",
    "                    iters = 0,            # Number of iterations (one of epochs or iters should be nonzero for training)\n",
    "                    bleu = false,         # Whether to calculate the BLEU score for the final model\n",
    "                    save = false,         # Whether to save the final model\n",
    "                    seconds = 60,         # Frequency of progress reporting\n",
    "                    )\n",
    "    @show bidirectional, layers, hidden, srcembed, tgtembed, dropout, epochs, iters, bleu, save; flush(stdout)\n",
    "    model = S2S(hidden, srcembed, tgtembed, trn.src.vocab, trn.tgt.vocab;\n",
    "                layers=layers, dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "    epochs == iters == 0 && return model\n",
    "\n",
    "    (ctrn,cdev,ctst) = collect(trn),collect(dev),collect.(tst)\n",
    "    traindata = (epochs > 0\n",
    "                 ? collect(flatten(shuffle!(ctrn) for i in 1:epochs))\n",
    "                 : shuffle!(collect(take(cycle(ctrn), iters))))\n",
    "\n",
    "    bestloss, bestmodel = loss(model, cdev), deepcopy(model)\n",
    "    progress!(adam(model, traindata), seconds=seconds) do y\n",
    "        devloss = loss(model, cdev)\n",
    "        tstloss = map(d->loss(model,d), ctst)\n",
    "        if devloss < bestloss\n",
    "            bestloss, bestmodel = devloss, deepcopy(model)\n",
    "        end\n",
    "        println(stderr)\n",
    "        (dev=devloss, tst=tstloss, mem=Float32(CuArrays.usage[]))\n",
    "    end\n",
    "    save && Knet.save(\"bestmodel.jld2\", \"bestmodel\", bestmodel)\n",
    "    bleu && Main.bleu(bestmodel,dev)\n",
    "    return bestmodel\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model: If your implementation is correct, the first epoch should take about 24\n",
    "minutes on a v100 and bring the loss from 9.83 to under 4.0. 10 epochs would take about 4\n",
    "hours on a v100. With other GPUs you may have to use a smaller batch size (if memory is\n",
    "lower) and longer time (if gpu speed is lower)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Knet.save(\"bestmodel.jld2\", \"bestmodel\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S(Embed(P(KnetArray{Float32,2}(512,38126))), LSTM(input=512,hidden=512,bidirectional,dropout=0.2), Memory(P(KnetArray{Float32,2}(512,1024))), Embed(P(KnetArray{Float32,2}(512,18857))), LSTM(input=1024,hidden=512,layers=2,dropout=0.2), Attention(1, P(KnetArray{Float32,2}(512,1536)), P(KnetArray{Float32,1}(1))), Linear(P(KnetArray{Float32,2}(18857,512)), P(KnetArray{Float32,1}(18857))), 0.2, Vocab(Dict(\"ağacından\" => 35370,\"komuta\" => 13566,\"ellisi\" => 25239,\"adresini\" => 22820,\"yüzeyi\" => 4051,\"paris'te\" => 9494,\"kafamdaki\" => 18790,\"yüzeyinde\" => 5042,\"geçerlidir\" => 6612,\"kökten\" => 7774…), [\"<s>\", \"<unk>\", \".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\"  …  \"seçmemiz\", \"destekleyip\", \"karşılaştırılabilir\", \"ördeğin\", \"gününüzü\", \"bağışçı\", \"istismara\", \"yaşça\", \"tedci\", \"fakültesi'nde\"], 2, 1, split), Vocab(Dict(\"middle-income\" => 13398,\"photosynthesis\" => 7689,\"polarizing\" => 17881,\"henry\" => 4248,\"abducted\" => 15691,\"rises\" => 6225,\"hampshire\" => 13888,\"whiz\" => 16835,\"cost-benefit\" => 13137,\"progression\" => 5549…), [\"<s>\", \"<unk>\", \",\", \".\", \"the\", \"and\", \"to\", \"of\", \"a\", \"that\"  …  \"archaea\", \"handshake\", \"brit\", \"wiper\", \"heroines\", \"coca\", \"exceptionally\", \"gallbladder\", \"autopsies\", \"linguistics\"], 2, 1, split))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestmodel = Knet.load(\"bestmodel.jld2\")[\"bestmodel\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┣████████████████████┫ [100.00%, 132/132, 00:31/00:31, 4.29i/s]                  ┫ [7.58%, 10/132, 00:02/00:32, 3.79i/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "perl: warning: Setting locale failed.\n",
      "perl: warning: Please check that your locale settings:\n",
      "\tLANGUAGE = (unset),\n",
      "\tLC_ALL = (unset),\n",
      "\tLC_CTYPE = \"UTF-8\",\n",
      "\tLANG = \"en_US.UTF-8\"\n",
      "    are supported and installed on your system.\n",
      "perl: warning: Falling back to the standard locale (\"C\").\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 17.94, 48.9/22.1/12.3/6.1 (BP=1.000, ratio=1.035, hyp_len=85399, ref_len=82502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"/tmp/jl_yz3Pb7\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu(bestmodel, ddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to sample translations from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "translate_sample (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = MTData(tr_dev, en_dev, batchsize=1) |> collect;\n",
    "function translate_sample(model, data)\n",
    "    (src,tgt) = rand(data)\n",
    "    out = bestmodel(src)\n",
    "    println(\"SRC: \", int2str(src,model.srcvocab))\n",
    "    println(\"REF: \", int2str(tgt,model.tgtvocab))\n",
    "    println(\"OUT: \", int2str(out,model.tgtvocab))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate translations for random instances from the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: birinci sırada ne var ?\n",
      "REF: what 's number one ?\n",
      "OUT: what is next ?\n"
     ]
    }
   ],
   "source": [
    "translate_sample(bestmodel, data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to generate translations from user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "translate_input (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function translate_input(model)\n",
    "    v = model.srcvocab\n",
    "    src = [ get(v.w2i, w, v.unk) for w in v.tokenizer(readline()) ]'\n",
    "    out = model(src)\n",
    "    println(\"SRC: \", int2str(src,model.srcvocab))\n",
    "    println(\"OUT: \", int2str(out,model.tgtvocab))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate translations for user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdin> derin öğrenme çok zaman alıyor\n",
      "SRC: derin öğrenme çok zaman alıyor\n",
      "OUT: deep learning takes a lot of time .\n"
     ]
    }
   ],
   "source": [
    "translate_input(bestmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition\n",
    "\n",
    "The reference model `pretrained` has 16.2 bleu. By playing with the optimization algorithm\n",
    "and hyperparameters, using per-sentence loss, and (most importantly) splitting the Turkish\n",
    "words I was able to push the performance to 21.0 bleu. I will give extra credit to groups\n",
    "that can exceed 21.0 bleu in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
