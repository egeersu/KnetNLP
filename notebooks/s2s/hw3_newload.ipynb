{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "**Reference:** Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" In Advances in neural information processing systems, pp. 3104-3112. 2014. ([Paper](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks), [Sample code](https://github.com/tensorflow/nmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "@size (macro with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pkg\n",
    "packages = [\"Knet\", \"Test\", \"IterTools\", \"Random\"]\n",
    "for p in packages; Pkg.add(p); end\n",
    "Pkg.update(\"Knet\")\n",
    "using Knet, Test, Base.Iterators, IterTools, Random # , LinearAlgebra, StatsBase\n",
    "using AutoGrad: @gcheck  # to check gradients, usew with Float64\n",
    "Knet.atype() = KnetArray{Float32}  # determines what Knet.param() uses.\n",
    "macro size(z, s); esc(:(@assert (size($z) == $s) string(summary($z),!=,$s))); end # for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part -1. Types from the last project\n",
    "\n",
    "Please copy the following types and related functions from the last project: `Vocab`,\n",
    "`TextReader`, `Embed`, `Linear`, `mask!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "include(\"hw2_functions.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Load data\n",
    "\n",
    "We will use the Turkish-English pair from the [TED Talks Dataset](https://github.com/neulab/word-embeddings-for-nmt) for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing data\n",
      "└ @ Main In[3]:17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = \"datasets/tr_to_en\"\n",
    "\n",
    "if !isdir(datadir);\n",
    "    download(\"http://www.phontron.com/data/qi18naacl-dataset.tar.gz\", \"qi18naacl-dataset.tar.gz\")\n",
    "    run(`tar xzf qi18naacl-dataset.tar.gz`)\n",
    "end\n",
    "\n",
    "if !isdefined(Main, :tr_vocab)\n",
    "    tr_vocab = Vocab(\"$datadir/tr.train\", mincount=5)\n",
    "    en_vocab = Vocab(\"$datadir/en.train\", mincount=5)\n",
    "    tr_train = TextReader(\"$datadir/tr.train\", tr_vocab)\n",
    "    en_train = TextReader(\"$datadir/en.train\", en_vocab)\n",
    "    tr_dev = TextReader(\"$datadir/tr.dev\", tr_vocab)\n",
    "    en_dev = TextReader(\"$datadir/en.dev\", en_vocab)\n",
    "    tr_test = TextReader(\"$datadir/tr.test\", tr_vocab)\n",
    "    en_test = TextReader(\"$datadir/en.test\", en_vocab)\n",
    "    @info \"Testing data\"\n",
    "    @test length(tr_vocab.i2w) == 38126\n",
    "    @test length(first(tr_test)) == 16\n",
    "    @test length(collect(tr_test)) == 5029\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Minibatching\n",
    "\n",
    "For minibatching we are going to design a new iterator: `MTData`. This iterator is built\n",
    "on top of two TextReaders `src` and `tgt` that produce parallel sentences for source and\n",
    "target languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct MTData\n",
    "    src::TextReader        # reader for source language data\n",
    "    tgt::TextReader        # reader for target language data\n",
    "    batchsize::Int         # desired batch size\n",
    "    maxlength::Int         # skip if source sentence above maxlength\n",
    "    batchmajor::Bool       # batch dims (B,T) if batchmajor=false (default) or (T,B) if true.\n",
    "    bucketwidth::Int       # batch sentences with length within bucketwidth of each other\n",
    "    buckets::Vector        # sentences collected in separate arrays called buckets for each length range\n",
    "    batchmaker::Function   # function that turns a bucket into a batch.\n",
    "end\n",
    "\n",
    "function MTData(src::TextReader, tgt::TextReader; batchmaker = arraybatch, batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 10, numbuckets = min(128, maxlength ÷ bucketwidth))\n",
    "    buckets = [ [] for i in 1:numbuckets ] # buckets[i] is an array of sentence pairs with similar length\n",
    "    MTData(src, tgt, batchsize, maxlength, batchmajor, bucketwidth, buckets, batchmaker)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{MTData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{MTData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{MTData}) = NTuple{2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iterate(::MTData)\n",
    "\n",
    "Define the `iterate` function for the `MTData` iterator. `iterate` should return a\n",
    "`(batch, state)` pair or `nothing` if there are no more batches.  The `batch` is a\n",
    "`(x::Matrix{Int},y::Matrix{Int})` pair where `x` is a `(batchsize,srclength)` batch of\n",
    "source language sentences and `y` is a `(batchsize,tgtlength)` batch of parallel target\n",
    "language translations. The `state` is a pair of `(src_state,tgt_state)` which can be used\n",
    "to iterate `d.src` and `d.tgt` to get more sentences.  `iterate(d)` without a second\n",
    "argument should initialize `d` by emptying its buckets and calling `iterate` on the inner\n",
    "iterators `d.src` and `d.tgt` without a state. Please review the documentation on\n",
    "iterators from the last project.\n",
    "\n",
    "To keep similar length sentences together `MTData` uses arrays of similar length sentence\n",
    "pairs called buckets.  Specifically, the `(src_sentence,tgt_sentence)` pairs coming from\n",
    "`src` and `tgt` are pushed into `d.buckets[i]` when the length of the source sentence is\n",
    "in the range `((i-1)*d.bucketwidth+1):(i*d.bucketwidth)`. When one of the buckets reaches\n",
    "`d.batchsize` `d.batchmaker` is called with the full bucket producing a 2-D batch, the\n",
    "bucket is emptied and the batch is returned. If `src` and `tgt` are exhausted the\n",
    "remaining partially full buckets are turned into batches and returned in any order. If the\n",
    "source sentence length is larger than `length(d.buckets)*d.bucketwidth`, the last bucket\n",
    "is used.\n",
    "\n",
    "Sentences above a certain length can be skipped using the `d.maxlength` field, and\n",
    "transposed `x,y` arrays can be produced using the `d.batchmajor` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.iterate(d::MTData, state=nothing)\n",
    "    # Your code here\n",
    "    numbuckets=length(d.buckets)\n",
    "    batchsize=d.batchsize\n",
    "    if (state===nothing)\n",
    "#         buckets=[ [] for i in 1:numbuckets ]\n",
    "#         d=MTData(d.src, d.tgt, d.batchsize, d.maxlength, d.batchmajor, d.bucketwidth, buckets , d.batchmaker)\n",
    "        d.buckets=[ [] for i in 1:numbuckets ]\n",
    "        src_sentence, src_state=iterate(d.src)\n",
    "        tgt_sentence, tgt_state=iterate(d.tgt)\n",
    "#         println(src_sentence)\n",
    "#         println(tgt_sentence)\n",
    "    else\n",
    "        src_state=state[1]\n",
    "        tgt_state=state[2]\n",
    "        x=iterate(d.src, src_state)\n",
    "        y=iterate(d.tgt, tgt_state)\n",
    "        if (x==nothing || y==nothing)\n",
    "            for i in 1:numbuckets\n",
    "                if length(d.buckets[i])>0\n",
    "                    batch=d.batchmaker(d, d.buckets[i])\n",
    "                    d.buckets[i]=[]\n",
    "                    return (batch, (src_state, tgt_state))\n",
    "                end\n",
    "            end\n",
    "            #print(d.buckets) this prints the buckets just before it ends we expect all empty buckets\n",
    "            return nothing\n",
    "        else\n",
    "            src_sentence,src_state=x\n",
    "            tgt_sentence,tgt_state=y\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    while (length(src_sentence)>d.maxlength)\n",
    "        #print(\"the src sentence is skipped\")\n",
    "        src_sentence, src_state=iterate(d.src, src_state)\n",
    "        tgt_sentence, tgt_state=iterate(d.tgt, tgt_state)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    for i in 1:numbuckets \n",
    "        if (length(src_sentence) > length(d.buckets)*d.bucketwidth)\n",
    "            push!(d.buckets[numbuckets], (src_sentence, tgt_sentence))\n",
    "            batch=d.batchmaker(d, d.buckets[numbuckets])\n",
    "            d.buckets[numbuckets]=[]\n",
    "            return (batch, (src_state, tgt_state))\n",
    "        elseif (length(src_sentence) in ((i-1)*d.bucketwidth+1):(i*d.bucketwidth))\n",
    "            push!(d.buckets[i], (src_sentence, tgt_sentence))\n",
    "            if (length(d.buckets[i])==d.batchsize)\n",
    "                batch=d.batchmaker(d, d.buckets[i])\n",
    "                d.buckets[i]=[]\n",
    "                return (batch, (src_state, tgt_state))\n",
    "            else\n",
    "                return iterate(d, (src_state, tgt_state))\n",
    "            end\n",
    "        end         \n",
    "    end\n",
    "end\n",
    "#you still havent done batchmajor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arraybatch\n",
    "\n",
    "Define `arraybatch(d, bucket)` to be used as the default `d.batchmaker`. `arraybatch`\n",
    "takes an `MTData` object and an array of sentence pairs `bucket` and returns a\n",
    "`(x::Matrix{Int},y::Matrix{Int})` pair where `x` is a `(batchsize,srclength)` batch of\n",
    "source language sentences and `y` is a `(batchsize,tgtlength)` batch of parallel target\n",
    "language translations. Note that the sentences in the bucket do not have any `eos` tokens\n",
    "and they may have different lengths. `arraybatch` should copy the source sentences into\n",
    "`x` padding shorter ones on the left with `eos` tokens. It should copy the target\n",
    "sentences into `y` with an `eos` token in the beginning and end of each sentence and\n",
    "shorter sentences padded on the right with extra `eos` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arraybatch (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function arraybatch(d::MTData, bucket)\n",
    "    # Your code here\n",
    "    longest_src=0\n",
    "    longest_tgt=0\n",
    "    batchsize=length(bucket)\n",
    "#     x=[[] for i in 1:batchsize]\n",
    "#     y=[[] for i in 1:batchsize]\n",
    "    for sentence_pairs in bucket\n",
    "        if (length(sentence_pairs[1])>longest_src)\n",
    "            longest_src=length(sentence_pairs[1])\n",
    "        end\n",
    "        if (length(sentence_pairs[2])>longest_tgt)\n",
    "            longest_tgt=length(sentence_pairs[2])\n",
    "        end\n",
    "    end\n",
    "#     print(longest_src)\n",
    "#     print(longest_tgt)\n",
    "    x= Array{Int64,2}(undef, batchsize, longest_src)\n",
    "    y= Array{Int64,2}(undef, batchsize, longest_tgt+2)\n",
    "    for (i, sentence_pair) in enumerate(bucket)\n",
    "        for k in 1:(longest_src-length(sentence_pair[1]))\n",
    "            prepend!(sentence_pair[1], d.src.vocab.eos)\n",
    "        end\n",
    "        for (j, word) in enumerate(sentence_pair[1])\n",
    "            x[i,j]=word\n",
    "        end\n",
    "#         print(x[i,:])\n",
    "        prepend!(sentence_pair[2],d.tgt.vocab.eos)\n",
    "        append!(sentence_pair[2], d.tgt.vocab.eos)\n",
    "        for j in 1:(longest_tgt+2-length(sentence_pair[2]))\n",
    "            append!(sentence_pair[2], d.tgt.vocab.eos)\n",
    "        end\n",
    "        for (j, word) in enumerate(sentence_pair[2])\n",
    "            y[i,j]=word\n",
    "        end\n",
    "#         print(y[i,:])\n",
    "    end\n",
    "#     print(longest_src)\n",
    "#     print(longest_tgt)\n",
    "#     return (reshape(x, (batchsize, longest_src)), reshape(y, (batchsize, longest_tgt)))\n",
    "#     print(size(x))\n",
    "    return (x,y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing MTData\n",
      "└ @ Main In[7]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing MTData\"\n",
    "dtrn = MTData(tr_train, en_train)\n",
    "ddev = MTData(tr_dev, en_dev)\n",
    "dtst = MTData(tr_test, en_test)\n",
    "\n",
    "x,y = first(dtst)\n",
    "@test length(collect(dtst)) == 48\n",
    "@test size.((x,y)) == ((128,10),(128,24))\n",
    "@test x[1,1] == tr_vocab.eos\n",
    "@test x[1,end] != tr_vocab.eos\n",
    "@test y[1,1] == en_vocab.eos\n",
    "@test y[1,2] != en_vocab.eos\n",
    "@test y[1,end] == en_vocab.eos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Sequence to sequence model without attention\n",
    "\n",
    "In this part we will define a simple sequence to sequence encoder-decoder model for\n",
    "machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct S2S_v1\n",
    "    srcembed::Embed     # source language embedding\n",
    "    encoder::RNN        # encoder RNN (can be bidirectional)\n",
    "    tgtembed::Embed     # target language embedding\n",
    "    decoder::RNN        # decoder RNN\n",
    "    projection::Linear  # converts decoder output to vocab scores\n",
    "    dropout::Real       # dropout probability to prevent overfitting\n",
    "    srcvocab::Vocab     # source language vocabulary\n",
    "    tgtvocab::Vocab     # target language vocabulary\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2S_v1 constructor\n",
    "\n",
    "Define the S2S_v1 constructor using your predefined layer types (Embed, Linear), and the\n",
    "Knet RNN type. Please review the RNN documentation using `@doc RNN`, paying attention to\n",
    "the following options in particular: `numLayers`, `bidirectional`, `dropout`, `dataType`,\n",
    "`usegpu`. The last two are important if you experiment with array types other than the\n",
    "default `KnetArray{Float32}`: make sure the RNNs use the same array type as the other\n",
    "layers. Note that if the encoder is bidirectional, its `numLayers` should be half of the\n",
    "decoder so that their hidden states match in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S_v1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function S2S_v1(hidden::Int,         # hidden size for both the encoder and decoder RNN\n",
    "                srcembsz::Int,       # embedding size for source language\n",
    "                tgtembsz::Int,       # embedding size for target language\n",
    "                srcvocab::Vocab,     # vocabulary for source language\n",
    "                tgtvocab::Vocab;     # vocabulary for target language\n",
    "                layers=1,            # number of layers\n",
    "                bidirectional=false, # whether encoder RNN is bidirectional\n",
    "                dropout=0)           # dropout probability\n",
    "    # Your code here\n",
    "\n",
    "    \n",
    "    # embeddings\n",
    "    vocab_size_source = length(srcvocab.w2i)\n",
    "    vocab_size_target = length(tgtvocab.w2i)\n",
    "    srcembed = Embed(vocab_size_source, srcembsz)\n",
    "    tgtembed = Embed(vocab_size_target, tgtembsz)\n",
    "\n",
    "    # encoder\n",
    "    # [Ex, B, Tx] -> [Hx, Bx, Tx]\n",
    "    # encoder.h will be [Hx, B]\n",
    "    # decoder\n",
    "    # [Ey, B, Ty] -> [Hy, B, Ty]\n",
    "    # decoder.h will be [Hy, B]\n",
    "    \n",
    "    if bidirectional\n",
    "        encoder = RNN(srcembsz, hidden; numLayers = layers,   dropout = dropout, bidirectional = true)\n",
    "        decoder = RNN(tgtembsz, hidden; numLayers = (layers*2), dropout = dropout, bidirectional = false)\n",
    "    else\n",
    "        encoder = RNN(srcembsz, hidden; numLayers = layers, dropout = dropout, bidirectional = false)\n",
    "        decoder = RNN(tgtembsz, hidden; numLayers = layers, dropout = dropout, bidirectional = false)\n",
    "    end\n",
    "    \n",
    "\n",
    "    \n",
    "    # projection\n",
    "    # [hidden, B * Ty] -> [vocab_size_target, B * Ty]\n",
    "    #projection = Linear(vocab_size_target, hidden)\n",
    "    projection = Linear(hidden, vocab_size_target)\n",
    "    \n",
    "    # return model\n",
    "    S2S_v1(srcembed, encoder, tgtembed, decoder, projection, dropout, srcvocab, tgtvocab)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2S_v1 loss function\n",
    "\n",
    "Define the S2S_v1 loss function that takes `src`, a source language minibatch, and `tgt`,\n",
    "a target language minibatch and returns either a `(total_loss, num_words)` pair if\n",
    "`average=false`, or `(total_loss/num_words)` average if `average=true`.\n",
    "\n",
    "Assume that `src` and `tgt` are integer arrays of size `(B,Tx)` and `(B,Ty)` respectively,\n",
    "where `B` is the batch size, `Tx` is the length of the longest source sequence, `Ty` is\n",
    "the length of the longest target sequence. The `src` sequences only contain words, the\n",
    "`tgt` sequences surround the words with `eos` tokens at the start and end. This allows\n",
    "columns `tgt[:,1:end-1]` to be used as the decoder input and `tgt[:,2:end]` as the desired\n",
    "decoder output.\n",
    "\n",
    "Assume any shorter sentences in the batches have been padded with extra `eos` tokens on\n",
    "the left for `src` and on the right for `tgt`. Don't worry about masking `src` for the\n",
    "encoder, it doesn't have a significant effect on the loss. However do mask `tgt` before\n",
    "`nll`: you do not want the padding tokens to be counted in the loss calculation.\n",
    "\n",
    "Please review `@doc RNN`: in particular the `r.c` and `r.h` fields can be used to get/set\n",
    "the cell and hidden arrays of an RNN (note that `0` and `nothing` act as special values).\n",
    "\n",
    "RNNs take a dropout value at construction and apply dropout to the input of every layer if\n",
    "it is non-zero. You need to handle dropout for other layers in the loss function or in\n",
    "layer definitions as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S_v1)(src, tgt; average=true)\n",
    "    \n",
    "    # init encoder\n",
    "    s.encoder.h = 0\n",
    "    s.decoder.c = 0\n",
    "\n",
    "    # ENCODER\n",
    "    src_embed_out = s.srcembed(src)  \n",
    "    #println(\"source embed out: \", summary(src_embed_out), size(src_embed_out))\n",
    "    encoder_out = s.encoder(src_embed_out)\n",
    "    #println(\"encoder_out:\" , summary(encoder_out), size(encoder_out))\n",
    "\n",
    "    # init decoder with encoder's h and c\n",
    "    s.decoder.h = s.encoder.h\n",
    "    s.decoder.c = s.encoder.c\n",
    "\n",
    "    # DECODER\n",
    "    tgt_embed_out = s.tgtembed(tgt[:,1:end-1])\n",
    "    #println(\"target embed: \", summary(tgt_embed_out), size(tgt_embed_out))\n",
    "    decoder_out = s.decoder(tgt_embed_out)\n",
    "    #println(\"decoder_out: \", summary(decoder_out), size(decoder_out))\n",
    "    \n",
    "    \n",
    "    # reshape\n",
    "    decoder_out = reshape(decoder_out, (size(decoder_out)[1], size(decoder_out)[2] * size(decoder_out)[3]))\n",
    "    #println(\"decoder_out (reshaped): \", summary(decoder_out), size(decoder_out))\n",
    "\n",
    "\n",
    "    # Linear\n",
    "    projection_out = s.projection(decoder_out)\n",
    "    #println(\"projection_out: \", summary(projection_out), size(projection_out))\n",
    "\n",
    "    # NLL\n",
    "    scores = projection_out \n",
    "    #println(\"tgt: \", summary(tgt), size(tgt))\n",
    "\n",
    "    answers = tgt[:,2:end]     \n",
    "    answers = mask!(answers, s.tgtvocab.eos)    \n",
    "\n",
    "    \n",
    "    answers = reshape(answers, (1, size(answers)[1] * size(answers)[2]))\n",
    "    #println(\"scores: \", summary(scores), size(scores))\n",
    "    #println(\"answers: \", summary(answers), size(answers))\n",
    "\n",
    "    nll(scores, answers; dims=1, average=average)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing S2S_v1\n",
      "└ @ Main In[11]:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m\u001b[1mTest Failed\u001b[22m\u001b[39m at \u001b[39m\u001b[1mIn[11]:7\u001b[22m\n",
      "  Expression: model(x, y; average=false) == (14097.471f0, 1432)\n",
      "   Evaluated: (14097.531f0, 1432) == (14097.471f0, 1432)\n"
     ]
    },
    {
     "ename": "Test.FallbackTestSetException",
     "evalue": "There was an error during testing",
     "output_type": "error",
     "traceback": [
      "There was an error during testing",
      "",
      "Stacktrace:",
      " [1] record(::Test.FallbackTestSet, ::Test.Fail) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Test/src/Test.jl:737",
      " [2] do_test(::Test.Returned, ::Expr) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Test/src/Test.jl:526",
      " [3] top-level scope at In[11]:5"
     ]
    }
   ],
   "source": [
    "@info \"Testing S2S_v1\"\n",
    "Knet.seed!(1)\n",
    "model = S2S_v1(512, 512, 512, tr_vocab, en_vocab; layers=2, bidirectional=true, dropout=0.2)\n",
    "(x,y) = first(dtst)\n",
    "## Your loss can be slightly different due to different ordering of words in the vocabulary.\n",
    "## The reference vocabulary starts with eos, unk, followed by words in decreasing frequency.\n",
    "@test model(x,y; average=false) == (14097.471f0, 1432)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss for a whole dataset\n",
    "\n",
    "Define a `loss(model, data)` which returns a `(Σloss, Nloss)` pair if `average=false` and\n",
    "a `Σloss/Nloss` average if `average=true` for a whole dataset. Assume that `data` is an\n",
    "iterator of `(x,y)` pairs such as `MTData` and `model(x,y;average)` is a model like\n",
    "`S2S_v1` that computes loss on a single `(x,y)` pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(model, data; average=true)\n",
    "    if average; Knet.mean(model(x,y) for (x,y) in data)\n",
    "    else\n",
    "        total = 0\n",
    "        counter = 0\n",
    "        for (x,y) in data; out = model(x,y; average=average); total += out[1]; counter += out[2]; end\n",
    "        return (total, counter)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing loss\n",
      "└ @ Main In[13]:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m\u001b[1mTest Failed\u001b[22m\u001b[39m at \u001b[39m\u001b[1mIn[13]:2\u001b[22m\n",
      "  Expression: loss(model, dtst, average=false) == (1.0429117f6, 105937)\n",
      "   Evaluated: (1.042915f6, 105937) == (1.0429117f6, 105937)\n"
     ]
    },
    {
     "ename": "Test.FallbackTestSetException",
     "evalue": "There was an error during testing",
     "output_type": "error",
     "traceback": [
      "There was an error during testing",
      "",
      "Stacktrace:",
      " [1] record(::Test.FallbackTestSet, ::Test.Fail) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Test/src/Test.jl:737",
      " [2] do_test(::Test.Returned, ::Expr) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.2/Test/src/Test.jl:526",
      " [3] top-level scope at In[13]:2"
     ]
    }
   ],
   "source": [
    "@info \"Testing loss\"\n",
    "@test loss(model, dtst, average=false) == (1.0429117f6, 105937)\n",
    "# Your loss can be slightly different due to different ordering of words in the vocabulary.\n",
    "# The reference vocabulary starts with eos, unk, followed by words in decreasing frequency.\n",
    "# Also, because we do not mask src, different batch sizes may lead to slightly different\n",
    "# losses. The test above gives (1.0429178f6, 105937) with batchsize==1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SGD_v1\n",
    "\n",
    "The following function can be used to train our model. `trn` is the training data, `dev`\n",
    "is used to determine the best model, `tst...` can be zero or more small test datasets for\n",
    "loss reporting. It returns the model that does best on `dev`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to get under 3.40 dev loss with the following settings in 10\n",
    "epochs. The training speed on a V100 is about 3 mins/epoch or 40K words/sec, K80 is about\n",
    "6 times slower. Using settings closer to the Luong paper (per-sentence loss rather than\n",
    "per-word loss, SGD with lr=1, gclip=1 instead of Adam), you can get to 3.17 dev loss in\n",
    "about 25 epochs. Using dropout and shuffling batches before each epoch significantly\n",
    "improve the dev loss. You can play around with hyperparameters but I doubt results will\n",
    "get much better without attention. To verify your training, here is the dev loss I\n",
    "observed at the beginning of each epoch in one training session:\n",
    "`[9.83, 4.60, 3.98, 3.69, 3.52, 3.41, 3.35, 3.32, 3.30, 3.31, 3.33]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = S2S_v1(512, 512, 512, tr_vocab, en_vocab; layers=2, bidirectional=true, dropout=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_params (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_params(model)\n",
    "    encoder_w = Knet.load(\"encoder_w.jld2\")[\"encoder_w\"]\n",
    "    decoder_w = Knet.load(\"decoder_w.jld2\")[\"decoder_w\"]\n",
    "    proj = Knet.load(\"proj.jld2\")[\"proj\"]\n",
    "    srcembed = Knet.load(\"srcmembed.jld2\")[\"srcembed\"]\n",
    "    tgtembed = Knet.load(\"tgtembed.jld2\")[\"tgtembed\"]\n",
    "    srcvocab = Knet.load(\"srcvocab.jld2\")[\"srcvocab\"]\n",
    "    tgtvocab = Knet.load(\"tgtvocab.jld2\")[\"tgtvocab\"]\n",
    "    dropout = Knet.load(\"dropout.jld2\")[\"dropout\"]\n",
    "    model.encoder.w = encoder_w\n",
    "    model.decoder.w = decoder_w\n",
    "    model.projection = proj\n",
    "    model.srcembed = srcembed\n",
    "    model.tgtembed = tgtembed\n",
    "    model.srcvocab = srcvocab\n",
    "    model.tgtvocab = tgtvocab\n",
    "    model.dropout = dropout\n",
    "    model\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_params(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train!(model, trn, dev, tst...)\n",
    "    #println(\"trn: \", summary(trn))\n",
    "    #println(\"dev: \", summary(dev))\n",
    "    #println(\"tst: \", summary(tst), \" content: \", summary(tst[1]))   \n",
    "    bestmodel, bestloss = deepcopy(model), loss(model, dev)    \n",
    "    progress!(adam(model, trn), steps=100) do y\n",
    "        losses = [ loss(model, d) for d in (dev,tst...) ]\n",
    "        if losses[1] < bestloss\n",
    "            bestmodel, bestloss = deepcopy(model), losses[1]\n",
    "        end\n",
    "        return (losses...,)\n",
    "    end\n",
    "    #return model\n",
    "    return bestmodel\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs = 2\n",
    "#ctrn = collect(dtrn)\n",
    "#trnx10 = collect(flatten(shuffle!(ctrn) for i in 1:epochs))\n",
    "#trn20 = ctrn[1:20]\n",
    "#dev38 = collect(ddev);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = train!(model, trnx10, dev38, trn20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "save_model (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function save_model(model)\n",
    "    model_name = \"s2s_v1.jld2\"\n",
    "    Knet.save(\"encoder_w.jld2\", \"encoder_w\", model.encoder.w)\n",
    "    Knet.save(\"decoder_w.jld2\", \"decoder_w\", model.decoder.w)\n",
    "    Knet.save(\"proj.jld2\", \"proj\", model.projection)\n",
    "    Knet.save(\"srcmembed.jld2\", \"srcembed\", model.srcembed)\n",
    "    Knet.save(\"tgtembed.jld2\", \"tgtembed\", model.tgtembed)\n",
    "    Knet.save(\"srcvocab.jld2\", \"srcvocab\", model.srcvocab)\n",
    "    Knet.save(\"tgtvocab.jld2\", \"tgtvocab\", model.tgtvocab)\n",
    "    Knet.save(\"dropout.jld2\", \"dropout\", model.dropout)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating translations\n",
    "\n",
    "With a single argument, a `S2S_v1` object should take it as a batch of source sentences\n",
    "and generate translations for them. After passing `src` through the encoder and copying\n",
    "its hidden states to the decoder, the decoder is run starting with an initial input of all\n",
    "`eos` tokens. Highest scoring tokens are appended to the output and used as input for the\n",
    "subsequent decoder steps.  The decoder should stop generating when all sequences in the\n",
    "batch have generated `eos` or when `stopfactor * size(src,2)` decoder steps are reached. A\n",
    "correctly shaped target language batch should be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S_v1)(src::Matrix{Int}; stopfactor = 3)\n",
    "    \n",
    "    #get source embeddings\n",
    "    source_embed_out = s.srcembed(src)\n",
    "    \n",
    "    #init encoder\n",
    "    s.encoder.h = 0\n",
    "    s.encoder.c = 0\n",
    "\n",
    "    #get encoder out\n",
    "    encoder_out = s.encoder(source_embed_out)\n",
    "    \n",
    "    #set decoder to encoder out\n",
    "    s.decoder.h = 0\n",
    "    s.decoder.h = deepcopy(s.encoder.h)\n",
    "    s.decoder.c = 0\n",
    "    s.decoder.c = deepcopy(s.encoder.c)\n",
    "    \n",
    "    # get number of sentences in the batch\n",
    "    batch_size = size(src)[1]\n",
    "    \n",
    "    # calculate stopping condition\n",
    "    max_iters = stopfactor * size(src,2)\n",
    "    \n",
    "    # for each sentence, start with [eos]\n",
    "    outputs = rand(s.tgtvocab.eos:s.tgtvocab.eos, (batch_size, 1))\n",
    "        \n",
    "    for i in 1:max_iters\n",
    "    \n",
    "        #println(\"iter \", i, \" outputs: \", size(outputs))\n",
    "        \n",
    "        # get the embeddings of the current outputs, but only use the last timestep\n",
    "        target_embed_out = s.tgtembed(outputs[:,i])\n",
    "        #println(\"target embeddings:\", size(target_embed_out))\n",
    "        \n",
    "        \n",
    "        # decoder forward pass\n",
    "        decoder_out = s.decoder(target_embed_out)\n",
    "        #println(\"decoder output:\", size(decoder_out))\n",
    "        \n",
    "        \n",
    "        # projection forward pass\n",
    "        proj_out = s.projection(decoder_out)\n",
    "        \n",
    "        # eliminate <unk>\n",
    "        proj_out[s.tgtvocab.unk,1] = -10000\n",
    "\n",
    "        #println(\"projection output: \", size(proj_out))\n",
    "        \n",
    "        #get best words\n",
    "        #best_words should be (13,1)\n",
    "\n",
    "        best_words = (x->x[1]).(argmax(proj_out; dims=1))\n",
    "            \n",
    "        outputs = hcat(outputs, transpose(best_words))\n",
    "         \n",
    "    end\n",
    "    #println(size(s.decoder.h))\n",
    "    \n",
    "    outputs\n",
    "\n",
    "    # DONT FORGET: for each sentence, only take the tokens up to eos.\n",
    "    # you might also attempt to stop at eos but its gonna be harder\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int2str (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility to convert int arrays to sentence strings\n",
    "function int2str(y,vocab)\n",
    "    y = vec(y)\n",
    "    ysos = findnext(w->!isequal(w,vocab.eos), y, 1)\n",
    "    ysos == nothing && return \"\"\n",
    "    yeos = something(findnext(isequal(vocab.eos), y, ysos), 1+length(y))\n",
    "    join(vocab.i2w[y[ysos:yeos-1]], \" \")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Generating some translations\n",
      "└ @ Main In[45]:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: şimdi bu konu bizi insanların ahlakı evrende yapmaya <unk> oldukları hareketlerin kaynağına götürüyor .\n",
      "REF: now , this brings us to the sorts of moves that people are apt to make in the moral sphere .\n",
      "OUT: so i 'm not sure that you can have a decent job for you . i 'm not going to be here .\n"
     ]
    }
   ],
   "source": [
    "@info \"Generating some translations\"\n",
    "d = MTData(tr_dev, en_dev, batchsize=1) |> collect\n",
    "(src,tgt) = rand(d)\n",
    "out = model(src)\n",
    "println(\"SRC: \", int2str(src,model.srcvocab))\n",
    "println(\"REF: \", int2str(tgt,model.tgtvocab))\n",
    "println(\"OUT: \", int2str(out,model.tgtvocab))\n",
    "# Here is a sample output:\n",
    "# SRC: çin'e 15 şubat 2006'da ulaştım .\n",
    "# REF: i made it to china on february 15 , 2006 .\n",
    "# OUT: i got to china , china , at the last 15 years ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating BLEU\n",
    "\n",
    "BLEU is the most commonly used metric to measure translation quality. The following should\n",
    "take a model and some data, generate translations and calculate BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bleu (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bleu(s2s,d::MTData)\n",
    "    d = MTData(d.src,d.tgt,batchsize=32)\n",
    "    reffile = d.tgt.file\n",
    "    hypfile,hyp = mktemp()\n",
    "    for (x,y) in progress(collect(d))\n",
    "        g = s2s(x)\n",
    "        for i in 1:size(y,1)\n",
    "            println(hyp, int2str(g[i,:], d.tgt.vocab))\n",
    "        end\n",
    "    end\n",
    "    close(hyp)\n",
    "    isfile(\"multi-bleu.perl\") || download(\"https://github.com/moses-smt/mosesdecoder/raw/master/scripts/generic/multi-bleu.perl\", \"multi-bleu.perl\")\n",
    "    run(pipeline(`cat $hypfile`,`perl multi-bleu.perl $reffile`))\n",
    "    return hypfile\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣████████████████████┫ [100.00%, 132/132, 00:53/00:53, 2.50i/s]  \n",
      "perl: warning: Setting locale failed.\n",
      "perl: warning: Please check that your locale settings:\n",
      "\tLANGUAGE = (unset),\n",
      "\tLC_ALL = (unset),\n",
      "\tLC_CTYPE = \"UTF-8\",\n",
      "\tLANG = \"en_US.UTF-8\"\n",
      "    are supported and installed on your system.\n",
      "perl: warning: Falling back to the standard locale (\"C\").\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 7.40, 15.3/13.4/6.4/4.1 (BP=1.000, ratio=1.530, hyp_len=83756, ref_len=82502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"/tmp/jl_KYRtcO\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu(model, ddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating dev BLEU takes about 45 secs on a V100. We get about 8.0 BLEU which is pretty\n",
    "low. As can be seen from the sample translations a loss of ~3+ (perplexity ~20+) or a BLEU\n",
    "of ~8 is not sufficient to generate meaningful translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the quality of translations we can use more training data, different training\n",
    "and model parameters, or preprocess the input/output: e.g. splitting Turkish words to make\n",
    "suffixes look more like English function words may help. Other architectures,\n",
    "e.g. attention and transformer, perform significantly better than this simple S2S model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
