{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m IterTools ─ v1.2.0\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [c8e1da08]\u001b[39m\u001b[92m + IterTools v1.2.0\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      " \u001b[90m [c8e1da08]\u001b[39m\u001b[92m + IterTools v1.2.0\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [37e2e46d]\u001b[39m\u001b[92m + LinearAlgebra \u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m SortingAlgorithms ─ v0.3.1\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m Missings ────────── v0.4.3\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m DataAPI ─────────── v1.1.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m StatsBase ───────── v0.32.0\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [2913bbd2]\u001b[39m\u001b[92m + StatsBase v0.32.0\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      " \u001b[90m [9a962f9c]\u001b[39m\u001b[92m + DataAPI v1.1.0\u001b[39m\n",
      " \u001b[90m [e1d29d7a]\u001b[39m\u001b[92m + Missings v0.4.3\u001b[39m\n",
      " \u001b[90m [a2af1166]\u001b[39m\u001b[92m + SortingAlgorithms v0.3.1\u001b[39m\n",
      " \u001b[90m [2913bbd2]\u001b[39m\u001b[92m + StatsBase v0.32.0\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [8dfed614]\u001b[39m\u001b[92m + Test \u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling IterTools [c8e1da08-722c-5040-9ed9-7db0dc04731e]\n",
      "└ @ Base loading.jl:1242\n",
      "┌ Info: Precompiling StatsBase [2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91]\n",
      "└ @ Base loading.jl:1242\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Packages = [\"IterTools\", \"LinearAlgebra\", \"LinearAlgebra\", \"StatsBase\", \"Test\"]\n",
    "for p in Packages; Pkg.add(p); end;\n",
    "using Knet, Base.Iterators, IterTools, LinearAlgebra, StatsBase, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro size(z, s); esc(:(@assert (size($z) == $s) string(summary($z),!=,$s))); end; # for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"nn4nlp-code/data/ptb\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const datadir = \"nn4nlp-code/data/ptb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'nn4nlp-code'...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Process(`\u001b[4mgit\u001b[24m \u001b[4mclone\u001b[24m \u001b[4mhttps://github.com/neubig/nn4nlp-code.git\u001b[24m`, ProcessExited(0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isdir(datadir) || run(`git clone https://github.com/neubig/nn4nlp-code.git`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Vocab(file::String; tokenizer=split, vocabsize=Inf, mincount=1, unk=\"<unk>\", eos=\"<s>\")\n",
    "    io = open(file, \"r\")\n",
    "    lines = readlines(io)\n",
    "    close(io)\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    #tokenize each line with the function:tokenizer, add each tokenized line to reviews\n",
    "    for line in lines; tokenized = push!(tokenizer(line), eos); push!(reviews, tokenized); end\n",
    "      \n",
    "    freq = Dict(); \n",
    "    \n",
    "    #iterate over all words, count how many times they occur by adding them to w2i\n",
    "    for review in reviews\n",
    "        for word in review\n",
    "            if word in keys(freq); freq[word] += 1; else; freq[word] = 1; end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #sort the dictionary based on word frequencies, returns an array\n",
    "    freq = sort(collect(freq), by=x->x[2])\n",
    "    \n",
    "    # Remove the least common word until we reach the specified vocabsize.\n",
    "    # Keep track of total removed values\n",
    "    total_removed = 0\n",
    "    while (length(freq) > vocabsize)\n",
    "        total_removed += freq[1][2]\n",
    "        popfirst!(freq) \n",
    "    end\n",
    "    \n",
    "    # keep only the words that occur >= mincount times\n",
    "    temp = []\n",
    "    for (word, count) in freq\n",
    "        if count >= mincount; push!(temp, (word, count)); end\n",
    "    end\n",
    "    freq = temp\n",
    "\n",
    "    #turn array back into dictionary\n",
    "    freq = Dict{String, Int}(freq)\n",
    "    \n",
    "    #add total removed values to <unk>    \n",
    "    freq[unk] += total_removed\n",
    "    \n",
    "    # Create i2w\n",
    "    w2i = Dict(); i2w = []\n",
    "    for (i,elt) in enumerate(keys(freq))\n",
    "        w2i[elt] = i\n",
    "        push!(i2w, elt)\n",
    "    end\n",
    "    i2w = Vector{String}(i2w)\n",
    "    \n",
    "    Vocab(w2i, i2w, w2i[unk], w2i[eos], tokenizer)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing Vocab\n",
      "└ @ Main In[7]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing Vocab\"\n",
    "f = \"$datadir/train.txt\"\n",
    "v = Vocab(f)\n",
    "@test all(v.w2i[w] == i for (i,w) in enumerate(v.i2w))\n",
    "@test length(Vocab(f).i2w) == 10000\n",
    "@test length(Vocab(f, vocabsize=1234).i2w) == 1234\n",
    "@test length(Vocab(f, mincount=5).i2w) == 9859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words_to_ints (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function words_to_ints(vocab::Vocab, sentence)\n",
    "    [get(vocab.w2i, word, vocab.unk) for word in vocab.tokenizer(sentence)]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = Vocab(\"$datadir/train.txt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    if s==nothing\n",
    "        state=open(r.file)\n",
    "    else \n",
    "        state=s\n",
    "    end\n",
    "    if eof(state)\n",
    "        close(state)\n",
    "        return nothing\n",
    "    else\n",
    "        line= readline(state)\n",
    "        return words_to_ints(r.vocab, line), state\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# These are some optional functions that can be defined for iterators. They are required for\n",
    "# `collect` to work, which converts an iterator to a regular array.\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing TextReader\n",
      "└ @ Main In[12]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing TextReader\"\n",
    "train_sentences, valid_sentences, test_sentences =\n",
    "    (TextReader(\"$datadir/$file.txt\", train_vocab) for file in (\"train\",\"valid\",\"test\"))\n",
    "@test length(first(train_sentences)) == 24\n",
    "@test length(collect(train_sentences)) == 42068\n",
    "@test length(collect(valid_sentences)) == 3370\n",
    "@test length(collect(test_sentences)) == 3761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Embed; w; end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    Embed(param(embedsize,vocabsize))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    l.w[:,x]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing Embed\n",
      "└ @ Main In[14]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing Embed\"\n",
    "Knet.seed!(1)\n",
    "embed = Embed(100,10)\n",
    "embed.w\n",
    "input = rand(1:100, 2, 3)\n",
    "output = embed(input)\n",
    "\n",
    "@test size(output) == (10, 2, 3)\n",
    "@test norm(output) ≈ 0.59804f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Linear; w; b; end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    Linear(param(outputsize, inputsize), param0(outputsize))\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    l.w * mat(x,dims=1) .+ l.b\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing Linear\n",
      "└ @ Main In[16]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing Linear\"\n",
    "Knet.seed!(1)\n",
    "linear = Linear(100,10)\n",
    "input = oftype(linear.w, randn(Float32, 100, 5))\n",
    "output = linear(input)\n",
    "@test size(output) == (10, 5)\n",
    "@test norm(output) ≈ 5.5301356f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct NNLM; vocab; windowsize; embed; hidden; output; dropout; end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NNLM"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function NNLM(vocab::Vocab, windowsize::Int, embedsize::Int, hiddensize::Int, dropout::Real)\n",
    "    \n",
    "    vocabsize = length(vocab.w2i)\n",
    "\n",
    "    embed = Embed(vocabsize, embedsize)\n",
    "    hidden = Linear(windowsize * embedsize, hiddensize)\n",
    "    output = Linear(hiddensize, vocabsize)\n",
    "    \n",
    "    NNLM(vocab, windowsize, embed, hidden, output, dropout)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIST = 3\n",
    "EMBED = 128\n",
    "HIDDEN = 128\n",
    "DROPOUT = 0.5\n",
    "VOCAB = length(train_vocab.i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNLM(train_vocab, HIST, EMBED, HIDDEN, DROPOUT);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test model.vocab === train_vocab\n",
    "@test model.windowsize === HIST\n",
    "@test size(model.embed.w) == (EMBED,VOCAB)\n",
    "@test size(model.hidden.w) == (HIDDEN,HIST*EMBED)\n",
    "@test size(model.hidden.b) == (HIDDEN,)\n",
    "@test size(model.output.w) == (VOCAB,HIDDEN)\n",
    "@test size(model.output.b) == (VOCAB,)\n",
    "@test model.dropout == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred_v1 (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function pred_v1(m::NNLM, hist::AbstractVector{Int})\n",
    "    @assert length(hist) == m.windowsize \n",
    "    embeds = m.embed(hist)\n",
    "    embeds = reshape(embeds, size(embeds)[1] * size(embeds)[2])\n",
    "    embeds = dropout(embeds, m.dropout)\n",
    "    h = m.hidden(embeds)\n",
    "    h = dropout(h, m.dropout)\n",
    "    out = m.output(tanh.(h))\n",
    "    out = reshape(out, size(out)[1])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = repeat([model.vocab.eos], model.windowsize)\n",
    "p = pred_v1(model, h);\n",
    "\n",
    "@test size(p) == size(train_vocab.i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scores_v1 (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This predicts the scores for the whole sentence, will be used for later testing.\n",
    "function scores_v1(model, sent)\n",
    "    hist = repeat([ model.vocab.eos ], model.windowsize)\n",
    "    scores = []\n",
    "    for word in [ sent; model.vocab.eos ]\n",
    "        push!(scores, pred_v1(model, hist))\n",
    "        hist = [ hist[2:end]; word ]\n",
    "    end\n",
    "    hcat(scores...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate(m::NNLM; maxlength=30)\n",
    "    history = repeat([model.vocab.eos], model.windowsize)\n",
    "    sentence_indexes = []\n",
    "    sentence_words = []\n",
    "    \n",
    "    for i in 1:maxlength\n",
    "        scores = softmax(pred_v1(m, history))\n",
    "        scores[m.vocab.eos] = -10000\n",
    "        scores[m.vocab.unk] = -10000\n",
    "        \n",
    "        best_index = argmax(softmax(scores))\n",
    "        best_word = m.vocab.i2w[best_index]\n",
    "        \n",
    "        #handle history\n",
    "        popfirst!(history)\n",
    "        push!(history, i)\n",
    "        \n",
    "        #build the sentence\n",
    "        push!(sentence_indexes, best_index)\n",
    "        push!(sentence_words, best_word)\n",
    "    end\n",
    "    join(sentence_words, \" \")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"creek henry eaton shook toys steelworkers developers wpp inner-city rather\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess = generate(model; maxlength=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = generate(model; maxlength=5)\n",
    "@test s isa String\n",
    "@test length(split(s)) <= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_v1 (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss_v1(m::NNLM, sent::AbstractVector{Int}; average = true)\n",
    "    new_sent = deepcopy(sent)\n",
    "    push!(new_sent, m.vocab.eos)\n",
    "\n",
    "    pred = scores_v1(m, new_sent)[:,1:length(new_sent)]\n",
    "    if average; nll(pred, new_sent); else; nll(pred, new_sent; average=false); end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.2103f0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = first(train_sentences)\n",
    "avgloss = loss_v1(model,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = first(train_sentences)\n",
    "avgloss = loss_v1(model,s)\n",
    "(tot, cnt) = loss_v1(model, s, average = false)\n",
    "@test 9 < avgloss < 10\n",
    "@test cnt == length(s) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test tot/cnt ≈ avgloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maploss (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function maploss(lossfn, model, data; average = true)\n",
    "    if average\n",
    "        losses = []\n",
    "        for sent in data; push!(losses, lossfn(model, sent; average=average)); end\n",
    "        return sum(losses)/length(losses)\n",
    "    else\n",
    "        losses = []\n",
    "        for sent in data; push!(losses, lossfn(model, sent; average=average)); end \n",
    "        total_loss = sum((x->x[1]).(losses))\n",
    "        (total_loss, length(data) + sum(length.(data)))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.2103615f0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst100 = collect(take(test_sentences, 100))\n",
    "avgloss = maploss(loss_v1, model, tst100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst100 = collect(take(test_sentences, 100))\n",
    "avgloss = maploss(loss_v1, model, tst100)\n",
    "@test 9 < avgloss < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tot, cnt) = maploss(loss_v1, model, tst100, average = false)\n",
    "@test cnt == length(tst100) + sum(length.(tst100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test tot/cnt ≈ avgloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Timing loss_v1 with 1000 sentences\n",
      "└ @ Main In[37]:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37.973831 seconds (3.48 M allocations: 6.268 GiB, 3.93% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.2103615f0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Timing loss_v1 with 1000 sentences\"\n",
    "tst1000 = collect(take(test_sentences, 1000))\n",
    "@time maploss(loss_v1, model, tst1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Timing loss_v1 training with 100 sentences\n",
      "└ @ Main In[38]:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31.980503 seconds (13.60 M allocations: 23.567 GiB, 22.40% gc time)\n"
     ]
    }
   ],
   "source": [
    "@info \"Timing loss_v1 training with 100 sentences\"\n",
    "trn100 = ((model,x) for x in collect(take(train_sentences, 100)))\n",
    "@time sgd!(loss_v1, trn100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred_v2 (generic function with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function pred_v2(m::NNLM, hist::AbstractMatrix{Int})\n",
    "    embeds = m.embed(hist)\n",
    "    embeds = reshape(embeds, size(embeds)[1]*size(embeds)[2], size(embeds)[3])\n",
    "    embeds = dropout(embeds, m.dropout)\n",
    "    h = m.hidden(embeds)\n",
    "    h = tanh.(h)\n",
    "    h = dropout(h, m.dropout)\n",
    "    out = m.output(h)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scores_v2 (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function scores_v2(model, sent)\n",
    "    hist = [ repeat([ model.vocab.eos ], model.windowsize); sent ]\n",
    "    hist = vcat((hist[i:end+i-model.windowsize]' for i in 1:model.windowsize)...)\n",
    "    @assert size(hist) == (model.windowsize, length(sent)+1)\n",
    "    return pred_v2(model, hist)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = first(test_sentences)\n",
    "s1, s2 = scores_v1(model, sent), scores_v2(model, sent)\n",
    "@test size(s1) == size(s2) == (length(train_vocab.i2w), length(sent)+1)\n",
    "@test s1 ≈ s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_v2 (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss_v2(m::NNLM, sent::AbstractVector{Int}; average = true)\n",
    "    losses = []\n",
    "    push!(sent, m.vocab.eos)\n",
    "    pred = scores_v2(m, sent)\n",
    "    \n",
    "    if average\n",
    "        for i in (1:length(sent))\n",
    "            predi = pred[:,i]\n",
    "            lossi = nll(predi, [sent[i]])\n",
    "            push!(losses, lossi)\n",
    "        end\n",
    "        return sum(losses)/length(losses)\n",
    "        \n",
    "    else\n",
    "        for i in (1:length(sent))\n",
    "            predi = pred[:,i]\n",
    "            lossi = nll(predi, [sent[i]])\n",
    "            push!(losses, lossi)\n",
    "        end\n",
    "        return (sum(losses), length(losses))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = first(test_sentences)\n",
    "@test loss_v1(model, s) ≈ loss_v2(model, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst100 = collect(take(test_sentences, 100))\n",
    "@test maploss(loss_v1, model, tst100) ≈ maploss(loss_v2, model, tst100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Multiple sentences at a time (minibatching)\n",
    "To get even more performance out of a GPU we will process multiple sentences at a time. This is called minibatching and is unfortunately complicated by the fact that the sentences in a batch may not be of the same length. \n",
    "\n",
    "Let's first write the minibatched versions of `pred` and `loss`, and see how to batch sentences together later.\n",
    "\n",
    "## pred_v3\n",
    "\n",
    "`pred_v3` takes a model `m`, a N×B×S dimensional history array `hist`, and returns a V×B×S dimensional score array, where N is `m.windowsize`, V is the vocabulary size, B is the batch size, and S is maximum sentence length in the batch + 1 for the final eos token. \n",
    "\n",
    "First, the embeddings for all entries in `hist` are looked up, which results in an array of E×N×B×S where E is the embedding size. \n",
    "\n",
    "The embedding array is reshaped to (E*N)×(B*S) and dropout is applied. It is then fed to the hidden layer which returns a H×(B*S) hidden output where H is the hidden size. \n",
    "\n",
    "Following element-wise tanh and dropout, the output layer turns this into a score array of V×(B*S) which is reshaped and returned as a V×B×S dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred_v3 (generic function with 1 method)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model, hist -> out\n",
    "function pred_v3(m::NNLM, hist::Array{Int})\n",
    "    embeds = m.embed(hist)\n",
    "    embeds = reshape(embeds, size(embeds)[1]*size(embeds)[2],size(embeds)[3]*size(embeds)[4])\n",
    "    embeds = dropout(embeds, m.dropout)\n",
    "    h = m.hidden(embeds)\n",
    "    h = tanh.(h)\n",
    "    h = dropout(h, m.dropout)\n",
    "    out = m.output(h)\n",
    "    out = reshape(out, size(out)[1], size(hist)[2], size(hist)[3])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scores_v3 (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function scores_v3(model, sent)\n",
    "    hist = [ repeat([ model.vocab.eos ], model.windowsize); sent ]\n",
    "    hist = vcat((hist[i:end+i-model.windowsize]' for i in 1:model.windowsize)...)\n",
    "    @assert size(hist) == (model.windowsize, length(sent)+1)\n",
    "    hist = reshape(hist, size(hist,1), 1, size(hist,2))\n",
    "    return pred_v3(model, hist)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = first(train_sentences)\n",
    "scores_v3(model, sent);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test scores_v2(model, sent) ≈ scores_v3(model, sent)[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mask! (generic function with 1 method)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mask!(a,pad)\n",
    "    for row in 1:size(a)[1]\n",
    "        count = 0\n",
    "        for column in size(a)[2]:-1:1\n",
    "            if a[row,column] == pad; count+=1; else break; end\n",
    "        end\n",
    "        for column in size(a)[2]:-1:1\n",
    "            if count > 1; a[row,column] = 0; count-=1; end\n",
    "        end\n",
    "    end\n",
    "    a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1 2 1 1 1; 2 2 2 1 1; 1 1 2 2 2; 1 1 2 2 1]\n",
    "mask!(a,1)\n",
    "@test mask!(a,1) == [1 2 1 0 0; 2 2 2 1 0; 1 1 2 2 2; 1 1 2 2 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_v3 (generic function with 1 method)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss_v3(m::NNLM, batch::AbstractMatrix{Int}; average = true)\n",
    "\n",
    "    num_batch = size(batch)[1] \n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for b in 1:num_batch\n",
    "        sent = batch[b,:]\n",
    "        pred = scores_v3(m, sent)[:,:,1:length(sent)]\n",
    "        batch_loss = nll(pred, sent)        \n",
    "        push!(losses, batch_loss)\n",
    "    end\n",
    "    \n",
    "    if average\n",
    "        return sum(losses) / num_batch\n",
    "    else\n",
    "        return (sum(losses), length(losses))\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.103133f0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = first(test_sentences)\n",
    "b = [ s; model.vocab.eos ]'\n",
    "loss_v3(model, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test loss_v2(model, s) ≈ loss_v3(model, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Minibatching\n",
    "#\n",
    "# Below is a sample implementation of a sequence minibatcher. The `LMData` iterator wraps a\n",
    "# TextReader and produces batches of sentences with similar length to minimize padding (too\n",
    "# much padding wastes computation). To be able to scale to very large files, we do not want\n",
    "# to read the whole file, sort by length etc. Instead `LMData` keeps around a small number\n",
    "# of buckets and fills them with similar sized sentences from the TextReader. As soon as one\n",
    "# of the buckets reaches the desired batch size it is turned into a matrix with the\n",
    "# necessary padding and output. When the TextReader is exhausted the remaining buckets are\n",
    "# returned (which may have smaller batch sizes). I will let you figure the rest out from the\n",
    "# following, there is no code to write for this part.\n",
    "\n",
    "struct LMData\n",
    "    src::TextReader\n",
    "    batchsize::Int\n",
    "    maxlength::Int\n",
    "    bucketwidth::Int\n",
    "    buckets\n",
    "end\n",
    "\n",
    "function LMData(src::TextReader; batchsize = 64, maxlength = typemax(Int), bucketwidth = 10)\n",
    "    numbuckets = min(128, maxlength ÷ bucketwidth)\n",
    "    buckets = [ [] for i in 1:numbuckets ]\n",
    "    LMData(src, batchsize, maxlength, bucketwidth, buckets)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{LMData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{LMData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{LMData}) = Matrix{Int}\n",
    "\n",
    "function Base.iterate(d::LMData, state=nothing)\n",
    "    if state == nothing\n",
    "        for b in d.buckets; empty!(b); end\n",
    "    end\n",
    "    bucket,ibucket = nothing,nothing\n",
    "    while true\n",
    "        iter = (state === nothing ? iterate(d.src) : iterate(d.src, state))\n",
    "        if iter === nothing\n",
    "            ibucket = findfirst(x -> !isempty(x), d.buckets)\n",
    "            bucket = (ibucket === nothing ? nothing : d.buckets[ibucket])\n",
    "            break\n",
    "        else\n",
    "            sent, state = iter\n",
    "            if length(sent) > d.maxlength || length(sent) == 0; continue; end\n",
    "            ibucket = min(1 + (length(sent)-1) ÷ d.bucketwidth, length(d.buckets))\n",
    "            bucket = d.buckets[ibucket]\n",
    "            push!(bucket, sent)\n",
    "            if length(bucket) === d.batchsize; break; end\n",
    "        end\n",
    "    end\n",
    "    if bucket === nothing; return nothing; end\n",
    "    batchsize = length(bucket)\n",
    "    maxlen = maximum(length.(bucket))\n",
    "    batch = fill(d.src.vocab.eos, batchsize, maxlen + 1)\n",
    "    for i in 1:batchsize\n",
    "        batch[i, 1:length(bucket[i])] = bucket[i]\n",
    "    end\n",
    "    empty!(bucket)\n",
    "    return batch, state\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Timing loss_v2 and loss_v3 at various batch sizes\n",
      "└ @ Main In[70]:1\n",
      "┌ Info: loss_v2\n",
      "└ @ Main In[70]:2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 52.176930 seconds (1.95 M allocations: 19.044 GiB, 9.73% gc time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (loss_v3, 1)\n",
      "└ @ Main In[70]:5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45.423749 seconds (746.51 k allocations: 18.992 GiB, 8.34% gc time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: (loss_v3, 8)\n",
      "└ @ Main In[70]:5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46.731847 seconds (414.72 k allocations: 22.153 GiB, 8.46% gc time)\n",
      "\u001b[91m\u001b[1mTest Failed\u001b[22m\u001b[39m at \u001b[39m\u001b[1mIn[70]:6\u001b[22m\n",
      "  Expression: p3 ≈ p2\n",
      "   Evaluated: 8.996761f0 ≈ 9.0626745f0\n"
     ]
    },
    {
     "ename": "Test.FallbackTestSetException",
     "evalue": "There was an error during testing",
     "output_type": "error",
     "traceback": [
      "There was an error during testing",
      "",
      "Stacktrace:",
      " [1] record(::Test.FallbackTestSet, ::Test.Fail) at /Users/sabae/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.2/Test/src/Test.jl:737",
      " [2] do_test(::Test.Returned, ::Expr) at /Users/sabae/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.2/Test/src/Test.jl:526",
      " [3] top-level scope at ./In[70]:6"
     ]
    }
   ],
   "source": [
    "@info \"Timing loss_v2 and loss_v3 at various batch sizes\"\n",
    "@info loss_v2; test_collect = collect(test_sentences)\n",
    "GC.gc(); @time p2 = maploss(loss_v2, model, test_collect)\n",
    "for B in (1, 8, 16, 32, 64, 128, 256)\n",
    "    @info loss_v3,B; test_batches = collect(LMData(test_sentences, batchsize = B))\n",
    "    GC.gc(); @time p3 = maploss(loss_v3, model, test_batches); @test p3 ≈ p2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training, a batchsize around 64 seems best, although things are a bit more complicated\n",
    "# here: larger batch sizes make fewer updates per epoch which may slow down convergence. We\n",
    "# will use the smaller test data to get quick results.\n",
    "\n",
    "@info \"Timing SGD for loss_v2 and loss_v3 at various batch sizes\"\n",
    "train(loss, model, data) = sgd!(loss, ((model,sent) for sent in data))\n",
    "@info loss_v2; test_collect = collect(test_sentences)\n",
    "GC.gc(); @time train(loss_v2, model, test_collect)\n",
    "for B in (1, 8, 16, 32, 64, 128, 256)\n",
    "    @info loss_v3,B; test_batches = collect(LMData(test_sentences, batchsize = B))\n",
    "    GC.gc(); @time train(loss_v3, model, test_batches)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┣████████████████████┫ [100.00%, 13240/13240, 47:18/47:18, 4.66i/s] (trn = 58.026688f0, dev = 75.65292f0, ∇ = 0.3552466f0))█▏                 ┫ [10.98%, 1454/13240, 05:14/47:36, 4.53i/s] (trn = 119.91741f0, dev = 132.27208f0, ∇ = 0.28435707f0)████                ┫ [20.50%, 2714/13240, 09:44/47:27, 4.65i/s] (trn = 87.47838f0, dev = 102.39043f0, ∇ = 0.30593753f0)[21.90%, 2900/13240, 10:23/47:25, 4.80i/s] (trn = 91.07605f0, dev = 101.68594f0, ∇ = 0.31700698f0)████████████▉       ┫ [64.58%, 8551/13240, 30:32/47:16, 4.65i/s] (trn = 65.62978f0, dev = 79.62601f0, ∇ = 0.3490029f0)██████████████▋     ┫ [73.52%, 9734/13240, 34:47/47:19, 4.55i/s] (trn = 61.659767f0, dev = 78.60619f0, ∇ = 0.34601617f0)\n"
     ]
    }
   ],
   "source": [
    "# ## Part 7. Training\n",
    "#\n",
    "# You should be able to get the validation loss under 5.1 (perplexity under 165) in 100\n",
    "# epochs with default parameters.  This takes about 5 minutes on a V100 GPU.\n",
    "#\n",
    "# Please review Knet function `progress!` and iterator function `ncycle` used below.\n",
    "\n",
    "model = NNLM(train_vocab, HIST, EMBED, HIDDEN, DROPOUT)\n",
    "train_batches = collect(LMData(train_sentences))\n",
    "valid_batches = collect(LMData(valid_sentences))\n",
    "test_batches = collect(LMData(test_sentences))\n",
    "train_batches50 = train_batches[1:50] # Small sample for quick loss calculation\n",
    "\n",
    "epoch = adam(loss_v3, ((model, batch) for batch in train_batches))\n",
    "bestmodel, bestloss = deepcopy(model), maploss(loss_v3, model, valid_batches)\n",
    "\n",
    "\n",
    "progress!(ncycle(epoch, 20), seconds=5) do x\n",
    "    global bestmodel, bestloss\n",
    "    ## Report gradient norm for the first batch\n",
    "    f = @diff loss_v3(model, train_batches[1])\n",
    "    gnorm = sqrt(sum(norm(grad(f,x))^2 for x in params(model)))\n",
    "    ## Report training and validation loss\n",
    "    trnloss = maploss(loss_v3, model, train_batches50)\n",
    "    devloss = maploss(loss_v3, model, valid_batches)\n",
    "    ## Save model that does best on validation data\n",
    "    if devloss < bestloss\n",
    "        bestmodel, bestloss = deepcopy(model), devloss\n",
    "    end\n",
    "    (trn=exp(trnloss), dev=exp(devloss), ∇=gnorm)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the to the in for peters the to the and\""
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(bestmodel, maxlength=10)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
